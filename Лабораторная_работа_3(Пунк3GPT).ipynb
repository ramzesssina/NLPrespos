{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "authorship_tag": "ABX9TyPDM1sh8bsbAJJRL+xCNUgR",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/ramzesssina/NLPrespos/blob/main/%D0%9B%D0%B0%D0%B1%D0%BE%D1%80%D0%B0%D1%82%D0%BE%D1%80%D0%BD%D0%B0%D1%8F_%D1%80%D0%B0%D0%B1%D0%BE%D1%82%D0%B0_3(%D0%9F%D1%83%D0%BD%D0%BA3GPT).ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "# **Пункт 3 - Создание GPT**"
      ],
      "metadata": {
        "id": "6khsjHWJCG_f"
      }
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "9mArDtGcvXXR"
      },
      "outputs": [],
      "source": [
        "import numpy as np"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "def sigmoid(x):\n",
        "    x = np.clip(x, -500, 500)\n",
        "    return 1 / (1 + np.exp(-x))\n",
        "\n",
        "def deriv_sigmoid(x):\n",
        "    fx = sigmoid(x)\n",
        "    return fx * (1 - fx)"
      ],
      "metadata": {
        "id": "afNNFYKIv7VO"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Neuron:\n",
        "    def __init__(self, input_size):\n",
        "        self.w = np.random.randn(input_size) * 0.1\n",
        "        self.b = 0.0\n",
        "\n",
        "    def feedforward(self, x):\n",
        "        self.last_x = x\n",
        "        self.last_total = np.dot(x, self.w) + self.b\n",
        "        return sigmoid(self.last_total)\n",
        "\n",
        "    def train(self, grad_output, lr=0.01):\n",
        "        grad_total = deriv_sigmoid(self.last_total) * grad_output\n",
        "        self.w -= lr * grad_total * self.last_x\n",
        "        self.b -= lr * grad_total"
      ],
      "metadata": {
        "id": "ZpEBSt5Ow537"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "class Head:\n",
        "    def __init__(self, vocab_size, embed_dim, hidden_dim, lr=0.01):\n",
        "        self.embed_dim = embed_dim\n",
        "        self.hidden_dim = hidden_dim\n",
        "        self.vocab_size = vocab_size\n",
        "        self.lr = lr\n",
        "\n",
        "        self.W_k = np.random.randn(embed_dim, hidden_dim) * 0.1\n",
        "        self.W_q = np.random.randn(embed_dim, hidden_dim) * 0.1\n",
        "        self.W_v = np.random.randn(embed_dim, hidden_dim) * 0.1\n",
        "\n",
        "        self.W_out = np.random.randn(hidden_dim, vocab_size) * 0.1\n",
        "        self.b_out = np.zeros((vocab_size,))\n",
        "\n",
        "    @staticmethod\n",
        "    def softmax(x):\n",
        "        exp_x = np.exp(x - np.max(x, axis=-1, keepdims=True))\n",
        "        return exp_x / np.sum(exp_x, axis=-1, keepdims=True)\n",
        "\n",
        "    def forward(self, x):\n",
        "        self.x = x\n",
        "        B, T, C = x.shape\n",
        "\n",
        "        self.k = x @ self.W_k\n",
        "        self.q = x @ self.W_q\n",
        "        self.v = x @ self.W_v\n",
        "\n",
        "        self.wei = np.matmul(self.q, self.k.transpose(0, 2, 1)) / np.sqrt(self.hidden_dim)\n",
        "        mask = np.tril(np.ones((T, T)))\n",
        "        self.wei = np.where(mask == 1, self.wei, -1e9)\n",
        "        self.wei_softmax = self.softmax(self.wei)\n",
        "        self.out = np.matmul(self.wei_softmax, self.v)\n",
        "\n",
        "        self.logits = np.matmul(self.out, self.W_out) + self.b_out\n",
        "        self.probs = self.softmax(self.logits)\n",
        "        return self.probs\n",
        "\n",
        "    def train(self, dataset, targets, epochs=1000):\n",
        "        for epoch in range(epochs):\n",
        "            probs = self.forward(dataset)\n",
        "            loss = -np.mean(np.log(probs[np.arange(len(dataset))[:, None], np.arange(dataset.shape[1]), targets]))\n",
        "\n",
        "            dlogits = probs\n",
        "            dlogits[np.arange(len(dataset))[:, None], np.arange(dataset.shape[1]), targets] -= 1\n",
        "            dlogits /= len(dataset) * dataset.shape[1]\n",
        "\n",
        "            grad_W_out = self.out.reshape(-1, self.hidden_dim).T @ dlogits.reshape(-1, self.vocab_size)\n",
        "            grad_b_out = np.sum(dlogits, axis=(0, 1))\n",
        "\n",
        "            self.W_out -= self.lr * grad_W_out\n",
        "            self.b_out -= self.lr * grad_b_out\n",
        "\n",
        "            if epoch % 100 == 0:\n",
        "                print(f\"Epoch {epoch}: Loss = {loss:.4f}\")\n",
        "\n",
        "    def evaluate(self, text_ids, idx_to_word, embedding_matrix, steps=5, true_text=None, temperature=1.0):\n",
        "      generated_text = \"\".join([idx_to_word[i] for i in text_ids])\n",
        "      print(\"Начальный текст:\", generated_text)\n",
        "\n",
        "      for step in range(steps):\n",
        "          input_ids = np.array(text_ids[-4:])\n",
        "          embedded = embedding_matrix[input_ids].reshape(1, -1, self.embed_dim)\n",
        "          probs = self.forward(embedded)[0, -1]\n",
        "\n",
        "          probs = np.log(probs + 1e-8) / temperature\n",
        "          probs = np.exp(probs) / np.sum(np.exp(probs))\n",
        "\n",
        "          next_id = np.random.choice(len(probs), p=probs)\n",
        "          text_ids.append(next_id)\n",
        "          predicted_word = idx_to_word[next_id]  # Используем idx_to_word, а не idx_to_char\n",
        "          if true_text is not None and step + 1 < len(true_text):\n",
        "              actual_word = true_text[step + 1]\n",
        "              print(f\"Предсказанное слово: {predicted_word}, Реальное слово: {actual_word}\")\n",
        "          else:\n",
        "              print(f\"Предсказанное слово: {predicted_word}\")"
      ],
      "metadata": {
        "id": "8GFejZeS0Ksb"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "text = \"Вдохновляясь новыми технологиями, я решительно настроен углубить свои знания в веб-разработке и автоматизации, чтобы быстро достичь профессиональных успехов. Мое стремление к совершенству подкрепляется ежедневными занятиями и практическими проектами.\"\n",
        "words = text.split()\n",
        "\n",
        "word_to_idx = {word: i for i, word in enumerate(sorted(set(words)))}\n",
        "idx_to_word = {i: word for word, i in word_to_idx.items()}\n",
        "vocab_size = len(word_to_idx)"
      ],
      "metadata": {
        "id": "GePyUpDs31Ve"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "seq_len = 4\n",
        "X = []\n",
        "Y = []\n",
        "\n",
        "for i in range(len(words) - seq_len):\n",
        "    seq = words[i:i+seq_len]\n",
        "    target = words[i+1:i+seq_len+1]\n",
        "    X.append([word_to_idx[word] for word in seq])\n",
        "    Y.append([word_to_idx[word] for word in target])\n",
        "\n",
        "X = np.array(X)\n",
        "Y = np.array(Y)"
      ],
      "metadata": {
        "id": "rWYrMsWN4h2A"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "embedding_dim = 2\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.1\n",
        "X_embedded = embedding_matrix[X]"
      ],
      "metadata": {
        "id": "nxY65GoM9_7x"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "X = np.array(X)\n",
        "Y = np.array(Y)\n",
        "\n",
        "embedding_dim = 2\n",
        "embedding_matrix = np.random.randn(vocab_size, embedding_dim) * 0.1\n",
        "X_embedded = embedding_matrix[X]\n",
        "\n",
        "model.train(X_embedded, Y, epochs=1000)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "R1ZhPhbw7kG8",
        "outputId": "a13bbd4d-3e73-452d-878b-17456431f98b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Epoch 0: Loss = 3.2498\n",
            "Epoch 100: Loss = 3.2411\n",
            "Epoch 200: Loss = 3.2361\n",
            "Epoch 300: Loss = 3.2331\n",
            "Epoch 400: Loss = 3.2310\n",
            "Epoch 500: Loss = 3.2295\n",
            "Epoch 600: Loss = 3.2283\n",
            "Epoch 700: Loss = 3.2273\n",
            "Epoch 800: Loss = 3.2265\n",
            "Epoch 900: Loss = 3.2258\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "initial_text = \"Вдохновляясь\"\n",
        "text_ids = [word_to_idx[word] for word in initial_text.split()]\n",
        "\n",
        "true_text = text[len(initial_text):len(initial_text)+5]\n",
        "model.evaluate(text_ids, idx_to_word, embedding_matrix, true_text=true_text)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "y0JK8Q3m8IzY",
        "outputId": "fe0e9209-b827-4ed1-e921-5af56bb48e0b"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Начальный текст: Вдохновляясь\n",
            "Предсказанное слово: ежедневными, Реальное слово: н\n",
            "Предсказанное слово: и, Реальное слово: о\n",
            "Предсказанное слово: свои, Реальное слово: в\n",
            "Предсказанное слово: ежедневными, Реальное слово: ы\n",
            "Предсказанное слово: настроен\n"
          ]
        }
      ]
    }
  ]
}